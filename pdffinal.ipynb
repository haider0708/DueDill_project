{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mega Pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Mega Pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mega\n",
      "[nltk_data]     Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Extracting PDFs:   7%|▋         | 51/770 [00:04<01:55,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: could not parse color space (252 0 R)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PDFs: 100%|██████████| 770/770 [01:19<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Ensure nltk stopwords are downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load a pre-trained BERT model for embeddings\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Lightweight and efficient\n",
    "\n",
    "# Define categories (for reference)\n",
    "CATEGORIES = {\n",
    "    \"Whitepaper\": [\"whitepaper\", \"protocol\", \"consensus\", \"blockchain\"],\n",
    "    \"Research\": [\"research\", \"study\", \"academic\", \"analysis\"],\n",
    "    \"Regulatory\": [\"compliance\", \"regulation\", \"legal\", \"SEC\", \"AML\", \"KYC\"],\n",
    "    \"Project Documentation\": [\"API\", \"developer\", \"documentation\", \"smart contract\"],\n",
    "    \"Exchange Reports\": [\"trading\", \"market\", \"liquidity\", \"exchange\"],\n",
    "    \"ICO Documents\": [\"ICO\", \"IEO\", \"STO\", \"tokenomics\", \"fundraising\"],\n",
    "    \"NFT Reports\": [\"NFT\", \"non-fungible\", \"metaverse\", \"digital art\"],\n",
    "    \"Security Reports\": [\"security\", \"audit\", \"vulnerability\", \"hack\"],\n",
    "    \"DeFi Reports\": [\"DeFi\", \"liquidity pool\", \"yield farming\"],\n",
    "    \"Taxation Reports\": [\"tax\", \"accounting\", \"audit\", \"IRS\"],\n",
    "}\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "# Load PDFs and extract text\n",
    "pdf_folder = \"downloaded_pdfs\"  # Change this to your folder path\n",
    "data = []\n",
    "\n",
    "for filename in tqdm(os.listdir(pdf_folder), desc=\"Extracting PDFs\"):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(pdf_folder, filename)\n",
    "        text = extract_text(filepath)\n",
    "        if text:\n",
    "            data.append({\"filename\": filename, \"text\": text})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Manually label a small sample for training\n",
    "training_data = [\n",
    "    (\"whitepaper-v3.pdf\", \"Whitepaper\"),\n",
    "    (\"Cryptex_-_Staking_Report.pdf\", \"Report\"),\n",
    "    (\"guide-to-regulation-on-cryptocurrency-and-digital-token.pdf\", \"Regulatory\"),\n",
    "    (\"state-of-nft-marketplaces.pdf\", \"NFT Reports\"),\n",
    "]\n",
    "\n",
    "train_df = pd.DataFrame(training_data, columns=[\"filename\", \"category\"])\n",
    "train_df[\"text\"] = train_df[\"filename\"].apply(lambda x: extract_text(os.path.join(pdf_folder, x)))\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"label\"] = label_encoder.fit_transform(train_df[\"category\"])\n",
    "\n",
    "# Convert text to BERT embeddings\n",
    "train_embeddings = np.array(bert_model.encode(train_df[\"text\"].tolist(), convert_to_numpy=True))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_embeddings, train_df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# Predict categories for all PDFs\n",
    "df[\"embeddings\"] = df[\"text\"].apply(lambda x: bert_model.encode(x, convert_to_numpy=True))\n",
    "df[\"label\"] = df[\"embeddings\"].apply(lambda x: model.predict([x])[0])\n",
    "df[\"category\"] = label_encoder.inverse_transform(df[\"label\"])\n",
    "\n",
    "# Organize PDFs into categorized folders\n",
    "output_folder = \"classified_pdfs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for category in df[\"category\"].unique():\n",
    "    category_path = os.path.join(output_folder, category)\n",
    "    os.makedirs(category_path, exist_ok=True)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    src_path = os.path.join(pdf_folder, row[\"filename\"])\n",
    "    dst_path = os.path.join(output_folder, row[\"category\"], row[\"filename\"])\n",
    "    shutil.move(src_path, dst_path)\n",
    "\n",
    "print(\"Classification completed! PDFs are organized in 'classified_pdfs'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
